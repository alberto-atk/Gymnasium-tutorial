{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3731c0c6",
   "metadata": {},
   "source": [
    "# Pruebas básicas\n",
    "Una vez vistas las clases por las que está formada gymnasium y de que trata, se van a realizar varias pruebas con un entorno, en concreto CartPole para ver el funcionamiento general de la librería y sus posibilidades. <br>\n",
    "<img src=\"./media/cart_pole.gif\" width=\"350px\"/>\n",
    "El entorno *CartPole*, cumple las siguientes características:\n",
    "- **Espacio de acciones** \n",
    "    0. para mover el carro a la izquierda\n",
    "    1. para mover el carro a la derecha\n",
    "- **Espacio de observaciones**:\n",
    "    0. Posición del carro\n",
    "    1. Velocidad del carro\n",
    "    2. Ángulo del palo\n",
    "    3. Velocidad angular del palo\n",
    "    \n",
    "La misión del entorno, es mantener el palo encima del carro el máximo tiempo posible.\n",
    "\n",
    "En esta primera prueba, las acciones que se van a tomar van a ser random, es decir, no se va a seguir ninguna política ni ningún tipo de entrenamiento. Este ejemplo, es únicamente para ver como sería una salida típica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ef0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 20 timesteps\n",
      "Episodio terminado después de 21 timesteps\n",
      "Episodio terminado después de 14 timesteps\n",
      "Episodio terminado después de 23 timesteps\n",
      "Episodio terminado después de 18 timesteps\n",
      "Episodio terminado después de 10 timesteps\n",
      "Episodio terminado después de 16 timesteps\n",
      "Episodio terminado después de 18 timesteps\n",
      "Episodio terminado después de 20 timesteps\n",
      "Episodio terminado después de 13 timesteps\n",
      "Episodio terminado después de 16 timesteps\n",
      "Episodio terminado después de 11 timesteps\n",
      "Episodio terminado después de 25 timesteps\n",
      "Episodio terminado después de 11 timesteps\n",
      "Episodio terminado después de 36 timesteps\n",
      "Episodio terminado después de 22 timesteps\n",
      "Episodio terminado después de 13 timesteps\n",
      "Episodio terminado después de 32 timesteps\n",
      "Episodio terminado después de 32 timesteps\n",
      "Episodio terminado después de 15 timesteps\n",
      "Tiempo medio: 19.3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() #primer reset necesario\n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): #tiempo que está en ejecución el entorno\n",
    "        action = env.action_space.sample() #accion random\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated: #finalizada la ejecución del anterior\n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fb5f4",
   "metadata": {},
   "source": [
    "Una posible política para este problema sería que el palo intentase estar centrado, ya que la meta es mantenerse el máximo tiempo posible. Para ello, si el palo es mayor que 0, se estará cayendo a la derecha por lo que habrá que mover el carro a la derecha también. Un ejemplo de implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e9d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 38 timesteps\n",
      "Episodio terminado después de 55 timesteps\n",
      "Episodio terminado después de 46 timesteps\n",
      "Episodio terminado después de 41 timesteps\n",
      "Episodio terminado después de 49 timesteps\n",
      "Episodio terminado después de 40 timesteps\n",
      "Episodio terminado después de 40 timesteps\n",
      "Episodio terminado después de 52 timesteps\n",
      "Episodio terminado después de 40 timesteps\n",
      "Episodio terminado después de 38 timesteps\n",
      "Episodio terminado después de 31 timesteps\n",
      "Episodio terminado después de 40 timesteps\n",
      "Episodio terminado después de 46 timesteps\n",
      "Episodio terminado después de 49 timesteps\n",
      "Episodio terminado después de 39 timesteps\n",
      "Episodio terminado después de 38 timesteps\n",
      "Episodio terminado después de 39 timesteps\n",
      "Episodio terminado después de 25 timesteps\n",
      "Episodio terminado después de 48 timesteps\n",
      "Episodio terminado después de 42 timesteps\n",
      "Tiempo medio: 41.8\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d757b",
   "metadata": {},
   "source": [
    "Después de esta ejecución, se puede comprobar que se duplica el tiempo medio que aguanta el palo, por lo que la política escogida sería correcta. <br> <br>\n",
    "\n",
    "Otra forma de posible, sería que además del ángulo, también la velocidad angular (*observation[3]*) sea positiva, ya que en casos en los que este prácticamente equilibrado, se haría un movimiento que podría ser innecesario. Un ejemplo de esta política sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7009e9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 135 timesteps\n",
      "Episodio terminado después de 117 timesteps\n",
      "Episodio terminado después de 196 timesteps\n",
      "Episodio terminado después de 142 timesteps\n",
      "Episodio terminado después de 180 timesteps\n",
      "Episodio terminado después de 135 timesteps\n",
      "Episodio terminado después de 181 timesteps\n",
      "Episodio terminado después de 118 timesteps\n",
      "Episodio terminado después de 165 timesteps\n",
      "Episodio terminado después de 220 timesteps\n",
      "Episodio terminado después de 168 timesteps\n",
      "Episodio terminado después de 131 timesteps\n",
      "Episodio terminado después de 216 timesteps\n",
      "Episodio terminado después de 177 timesteps\n",
      "Episodio terminado después de 123 timesteps\n",
      "Episodio terminado después de 191 timesteps\n",
      "Episodio terminado después de 196 timesteps\n",
      "Episodio terminado después de 134 timesteps\n",
      "Episodio terminado después de 156 timesteps\n",
      "Episodio terminado después de 229 timesteps\n",
      "Tiempo medio: 165.5\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0 and observation[3] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7e5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb04f2c",
   "metadata": {},
   "source": [
    "Para aplicar Q-learning a este problema continuo, sería neceario discretizarlo para poder formar la tabla o utilizar Deep Q-learning. Para demostrar un caso de uso de Aprendizaje Reforzado combinado con la librería (uso común de la misma), se va a aplicar y así de esta misma forma, se puede comparar el resultado con el obtenido por las políticas anteriores.<br> <br>\n",
    "\n",
    "Las características del entorno, serían las comentadas anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e176a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class DQN():\n",
    "    def __init__(self, env):\n",
    "        self.max_episodes = 20000\n",
    "        self.max_actions = 99\n",
    "        \n",
    "        self.epsilon = 1.0 \n",
    "        self.max_epsilon = 1.0\n",
    "        self.min_epsilon = 0.01 \n",
    "        self.decay_rate = 0.005\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        \n",
    "        self.input_n = env.observation_space.shape[0]\n",
    "        self.output_n = env.action_space.n\n",
    "        \n",
    "        self.nn = self.nn_model(env)\n",
    "        self.env = env\n",
    "        \n",
    "        print(self.output_n)\n",
    "        \n",
    "    def nn_model(self, env):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.input_n, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(24, input_dim=self.output_n, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def act(self, observation):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.nn.predict(observation)[0]) \n",
    "            print(action)\n",
    "        #Se reduce epsilon\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.decay_rate\n",
    "        return action\n",
    "        \n",
    "    def train(self, episodes):\n",
    "        replay_buffer = []\n",
    "        reward_buffer = []\n",
    "        episode_reward = 0.0\n",
    "        for episode in range(episodes):\n",
    "            observation, info = env.reset()\n",
    "            for s in range(10000): \n",
    "\n",
    "                action = self.act(observation)\n",
    "                new_observation, reward, terminated, truncated, info = env.step(action)\n",
    "                transition = (observation, action, reward, terminated, new_observation)\n",
    "                replay_buffer.append(transition)\n",
    "                observation = new_observation\n",
    "\n",
    "                episode_reward += reward\n",
    "                \n",
    "\n",
    "                if s % 100 == 0:\n",
    "                    print('Step: {}'.format(s))\n",
    "                    print('Avg reward: {}'.format(np.mean(reward_buffer)))\n",
    "\n",
    "                if terminated or truncated: \n",
    "                    observation, info = env.reset()\n",
    "                    reward_buffer.append(episode_reward)\n",
    "                    episode_reward = 0\n",
    "                    break\n",
    "    \n",
    "\n",
    "    def test(self, Q):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9e7f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gymnasium as gym\\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\\nd = DQN(env)\\nd.train(1000)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "d = DQN(env)\n",
    "d.train(1000)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8fec936",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a9a1f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_6_input'), name='dense_6_input', description=\"created by layer 'dense_6_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n    \n    Input 0 of layer \"dense_6\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_2' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m observation, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m): \n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated: \n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mDQN.act\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     40\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#Se reduce epsilon\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filepy0c38x_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n    \n    Input 0 of layer \"dense_6\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_2' (type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "d = DQN(env)\n",
    "\n",
    "for episode in range(2):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        action = d.act(observation)\n",
    "            \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "     \n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccce7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
