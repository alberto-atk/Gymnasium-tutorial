{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3731c0c6",
   "metadata": {},
   "source": [
    "# Pruebas básicas\n",
    "Una vez vistas las clases por las que está formada gymnasium y de que trata, se van a realizar varias pruebas con un entorno, en concreto CartPole para ver el funcionamiento general de la librería y sus posibilidades. <br>\n",
    "\n",
    "El entorno *CartPole*, cumple las siguientes características:\n",
    "- **Espacio de acciones** \n",
    "    0. para mover el carro a la izquierda\n",
    "    1. para mover el carro a la derecha\n",
    "- **Espacio de observaciones**:\n",
    "    0. Posición del carro\n",
    "    1. Velocidad del carro\n",
    "    2. Ángulo del palo\n",
    "    3. Velocidad angular del palo\n",
    "    \n",
    "La misión del entorno, es mantener el palo encima del carro el máximo tiempo posible.\n",
    "\n",
    "En esta primera prueba, las acciones que se van a tomar van a ser random, es decir, no se va a seguir ninguna política ni ningún tipo de entrenamiento. Este ejemplo, es únicamente para ver como sería una salida típica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ef0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 10 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 16 timesteps\n",
      "Episodio terminado después de 11 timesteps\n",
      "Episodio terminado después de 23 timesteps\n",
      "Episodio terminado después de 19 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 17 timesteps\n",
      "Episodio terminado después de 12 timesteps\n",
      "Episodio terminado después de 19 timesteps\n",
      "Episodio terminado después de 24 timesteps\n",
      "Episodio terminado después de 23 timesteps\n",
      "Episodio terminado después de 14 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 32 timesteps\n",
      "Episodio terminado después de 28 timesteps\n",
      "Episodio terminado después de 57 timesteps\n",
      "Episodio terminado después de 42 timesteps\n",
      "Episodio terminado después de 67 timesteps\n",
      "Episodio terminado después de 18 timesteps\n",
      "Tiempo medio: 26.85\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() #primer reset necesario\n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): #tiempo que está en ejecución el entorno\n",
    "        action = env.action_space.sample() #accion random\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated: #finalizada la ejecución del anterior\n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fb5f4",
   "metadata": {},
   "source": [
    "Una posible política para este problema sería que el palo intentase estar centrado, ya que la meta es mantenerse el máximo tiempo posible. Para ello, si el palo es mayor que 0, se estará cayendo a la derecha por lo que habrá que mover el carro a la derecha también. Un ejemplo de implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e9d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 39 timesteps\n",
      "Episodio terminado después de 34 timesteps\n",
      "Episodio terminado después de 50 timesteps\n",
      "Episodio terminado después de 34 timesteps\n",
      "Episodio terminado después de 48 timesteps\n",
      "Episodio terminado después de 44 timesteps\n",
      "Episodio terminado después de 39 timesteps\n",
      "Episodio terminado después de 25 timesteps\n",
      "Episodio terminado después de 59 timesteps\n",
      "Episodio terminado después de 25 timesteps\n",
      "Episodio terminado después de 43 timesteps\n",
      "Episodio terminado después de 37 timesteps\n",
      "Episodio terminado después de 46 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 41 timesteps\n",
      "Episodio terminado después de 53 timesteps\n",
      "Episodio terminado después de 54 timesteps\n",
      "Episodio terminado después de 49 timesteps\n",
      "Episodio terminado después de 45 timesteps\n",
      "Tiempo medio: 41.75\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d757b",
   "metadata": {},
   "source": [
    "Después de esta ejecución, se puede comprobar que se duplica el tiempo medio que aguanta el palo, por lo que la política escogida sería correcta. <br> <br>\n",
    "\n",
    "Otra forma de posible, sería que además del ángulo, también la velocidad angular (*observation[3]*) sea positiva, ya que en casos en los que este prácticamente equilibrado, se haría un movimiento que podría ser innecesario. Un ejemplo de esta política sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7009e9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 113 timesteps\n",
      "Episodio terminado después de 272 timesteps\n",
      "Episodio terminado después de 160 timesteps\n",
      "Episodio terminado después de 189 timesteps\n",
      "Episodio terminado después de 151 timesteps\n",
      "Episodio terminado después de 230 timesteps\n",
      "Episodio terminado después de 130 timesteps\n",
      "Episodio terminado después de 193 timesteps\n",
      "Episodio terminado después de 113 timesteps\n",
      "Episodio terminado después de 128 timesteps\n",
      "Episodio terminado después de 134 timesteps\n",
      "Episodio terminado después de 113 timesteps\n",
      "Episodio terminado después de 186 timesteps\n",
      "Episodio terminado después de 107 timesteps\n",
      "Episodio terminado después de 184 timesteps\n",
      "Episodio terminado después de 106 timesteps\n",
      "Episodio terminado después de 188 timesteps\n",
      "Episodio terminado después de 195 timesteps\n",
      "Episodio terminado después de 137 timesteps\n",
      "Episodio terminado después de 191 timesteps\n",
      "Tiempo medio: 161.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0 and observation[3] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7e5369",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb04f2c",
   "metadata": {},
   "source": [
    "Para aplicar Q-learning a este problema continuo, sería neceario discretizarlo para poder formar la tabla. Al ser un proceso que no tiene que ver con el uso y funcionamiento de la librería, se va a aplicar este algoritmo con un problema con espacio de observaciones discreto (*Frozen Lake*), para demostrar un caso de uso de Aprendizaje Reforzado combinado con la librería.<br> <br>\n",
    "\n",
    "Las características de este entorno son:\n",
    "- **Espacio de acciones** \n",
    "    0. Moverse a la izquierda\n",
    "    1. Moverse a la derecha\n",
    "    2. Moverse hacia arriba\n",
    "    3. Moverse hacia abajo\n",
    "- **Espacio de observaciones**:\n",
    "    0. Valor representado por: $fila\\_actual*nfilas + columna\\_actual$ (empezando ambas desde 0). Por ejemplo, la posición 4x4 sería: $3*4 + 3 = 15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b895f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Take the action (a) and observe the outcome state(s') and reward (r)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m new_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# qtable[new_state,:] : all the actions we can take from new state\u001b[39;00m\n\u001b[1;32m     54\u001b[0m qtable[state, action] \u001b[38;5;241m=\u001b[39m qtable[state, action] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(qtable[new_state, :]) \u001b[38;5;241m-\u001b[39m qtable[state, action])\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "total_episodes = 15000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005             # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# 2 For life or until learning is stopped\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # qtable[new_state,:] : all the actions we can take from new state\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        # Our new state is state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done (if we're dead) : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "        \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            \n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
