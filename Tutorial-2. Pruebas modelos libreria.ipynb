{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3731c0c6",
   "metadata": {},
   "source": [
    "# Pruebas básicas\n",
    "Una vez vistas las clases por las que está formada gymnasium y de que trata, se van a realizar varias pruebas con un entorno, en concreto CartPole para ver el funcionamiento general de la librería y sus posibilidades. <br>\n",
    "\n",
    "El entorno *CartPole*, cumple las siguientes características:\n",
    "- **Espacio de acciones** \n",
    "    0. para mover el carro a la izquierda\n",
    "    1. para mover el carro a la derecha\n",
    "- **Espacio de observaciones**:\n",
    "    0. Posición del carro\n",
    "    1. Velocidad del carro\n",
    "    2. Ángulo del palo\n",
    "    3. Velocidad angular del palo\n",
    "    \n",
    "La misión del entorno, es mantener el palo encima del carro el máximo tiempo posible.\n",
    "\n",
    "En esta primera prueba, las acciones que se van a tomar van a ser random, es decir, no se va a seguir ninguna política ni ningún tipo de entrenamiento. Este ejemplo, es únicamente para ver como sería una salida típica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ef0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 23 timesteps\n",
      "Episodio terminado después de 33 timesteps\n",
      "Episodio terminado después de 19 timesteps\n",
      "Episodio terminado después de 29 timesteps\n",
      "Episodio terminado después de 40 timesteps\n",
      "Episodio terminado después de 9 timesteps\n",
      "Episodio terminado después de 24 timesteps\n",
      "Episodio terminado después de 38 timesteps\n",
      "Episodio terminado después de 19 timesteps\n",
      "Episodio terminado después de 43 timesteps\n",
      "Episodio terminado después de 19 timesteps\n",
      "Episodio terminado después de 22 timesteps\n",
      "Episodio terminado después de 15 timesteps\n",
      "Episodio terminado después de 22 timesteps\n",
      "Episodio terminado después de 14 timesteps\n",
      "Episodio terminado después de 21 timesteps\n",
      "Episodio terminado después de 17 timesteps\n",
      "Episodio terminado después de 30 timesteps\n",
      "Episodio terminado después de 20 timesteps\n",
      "Episodio terminado después de 10 timesteps\n",
      "Tiempo medio: 23.35\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() #primer reset necesario\n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): #tiempo que está en ejecución el entorno\n",
    "        action = env.action_space.sample() #accion random\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if terminated or truncated: #finalizada la ejecución del anterior\n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fb5f4",
   "metadata": {},
   "source": [
    "Una posible política para este problema sería que el palo intentase estar centrado, ya que la meta es mantenerse el máximo tiempo posible. Para ello, si el palo es mayor que 0, se estará cayendo a la derecha por lo que habrá que mover el carro a la derecha también. Un ejemplo de implementación sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e9d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 53 timesteps\n",
      "Episodio terminado después de 57 timesteps\n",
      "Episodio terminado después de 39 timesteps\n",
      "Episodio terminado después de 52 timesteps\n",
      "Episodio terminado después de 34 timesteps\n",
      "Episodio terminado después de 36 timesteps\n",
      "Episodio terminado después de 36 timesteps\n",
      "Episodio terminado después de 36 timesteps\n",
      "Episodio terminado después de 37 timesteps\n",
      "Episodio terminado después de 36 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 52 timesteps\n",
      "Episodio terminado después de 47 timesteps\n",
      "Episodio terminado después de 25 timesteps\n",
      "Episodio terminado después de 51 timesteps\n",
      "Episodio terminado después de 47 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 66 timesteps\n",
      "Episodio terminado después de 35 timesteps\n",
      "Episodio terminado después de 26 timesteps\n",
      "Tiempo medio: 41.75\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d757b",
   "metadata": {},
   "source": [
    "Después de esta ejecución, se puede comprobar que se duplica el tiempo medio que aguanta el palo, por lo que la política escogida sería correcta. <br> <br>\n",
    "\n",
    "Otra forma de posible, sería que además del ángulo, también la velocidad angular (*observation[3]*) sea positiva, ya que en casos en los que este prácticamente equilibrado, se haría un movimiento que podría ser innecesario. Un ejemplo de esta política sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7009e9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/gymnasium/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado después de 109 timesteps\n",
      "Episodio terminado después de 138 timesteps\n",
      "Episodio terminado después de 223 timesteps\n",
      "Episodio terminado después de 151 timesteps\n",
      "Episodio terminado después de 182 timesteps\n",
      "Episodio terminado después de 109 timesteps\n",
      "Episodio terminado después de 159 timesteps\n",
      "Episodio terminado después de 195 timesteps\n",
      "Episodio terminado después de 129 timesteps\n",
      "Episodio terminado después de 114 timesteps\n",
      "Episodio terminado después de 190 timesteps\n",
      "Episodio terminado después de 212 timesteps\n",
      "Episodio terminado después de 122 timesteps\n",
      "Episodio terminado después de 142 timesteps\n",
      "Episodio terminado después de 119 timesteps\n",
      "Episodio terminado después de 218 timesteps\n",
      "Episodio terminado después de 177 timesteps\n",
      "Episodio terminado después de 214 timesteps\n",
      "Episodio terminado después de 166 timesteps\n",
      "Episodio terminado después de 240 timesteps\n",
      "Tiempo medio: 165.45\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset() \n",
    "\n",
    "total = 0\n",
    "\n",
    "for episode in range(20):\n",
    "    observation, info = env.reset()\n",
    "    for t in range(1000): \n",
    "        if observation[2] > 0.0 and observation[3] > 0.0:\n",
    "            observation, reward, terminated, truncated, info = env.step(1)\n",
    "        else:\n",
    "            observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "        if terminated or truncated: \n",
    "            observation, info = env.reset()\n",
    "            total += (t+1)\n",
    "            print(\"Episodio terminado después de {} timesteps\".format((t+1)))\n",
    "            break\n",
    "            \n",
    "print(\"Tiempo medio: {}\".format(total/20))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb04f2c",
   "metadata": {},
   "source": [
    "Para aplicar Q-learning a este problema continuo, sería neceario discretizarlo para poder formar la tabla. Al ser un proceso que no tiene que ver con el uso y funcionamiento de la librería, se va a aplicar este algoritmo con un problema con espacio de observaciones discreto (*Frozen Lake*), para demostrar un caso de uso de Aprendizaje Reforzado combinado con la librería.<br> <br>\n",
    "\n",
    "Las características de este entorno son:\n",
    "- **Espacio de acciones** \n",
    "    0. Moverse a la izquierda\n",
    "    1. Moverse a la derecha\n",
    "    2. Moverse hacia arriba\n",
    "    3. Moverse hacia abajo\n",
    "- **Espacio de observaciones**:\n",
    "    0. Valor representado por: $fila\\_actual*nfilas + columna\\_actual$ (empezando ambas desde 0). Por ejemplo, la posición 4x4 sería: $3*4 + 3 = 15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98b895f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table before training:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Until the agent gets stuck in a hole or reaches the goal, keep training it\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Choose the action with the highest value in the current state\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mqtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(qtable[state])\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# If there's no best action (only zeros), take a random one\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "# We re-initialize the Q-table\n",
    "qtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 1000        # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "\n",
    "# List of outcomes to plot\n",
    "outcomes = []\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)\n",
    "\n",
    "# Training\n",
    "for _ in range(episodes):\n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "\n",
    "    # By default, we consider our outcome to be a failure\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(qtable[state]) > 0:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "            action = environment.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Update Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + \\\n",
    "                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        \n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # If we have a reward, it means that our outcome is a success\n",
    "        if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "\n",
    "print()\n",
    "print('===========================================')\n",
    "print('Q-table after training:')\n",
    "print(qtable)\n",
    "\n",
    "# Plot outcomes\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#efeeea')\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
